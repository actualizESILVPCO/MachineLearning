# Apply log transformation
y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)

print('Log transformation applied to target variable')
print(f'Original skewness: {y_train.skew():.2f}')
print(f'Log-transformed skewness: {y_train_log.skew():.2f}')

# Function to evaluate with log transformation
def evaluate_model_log(model, X_train, X_test, y_train_log, y_test_log, y_test_original, model_name):
    print('\n')
    print(f'{model_name} (with Log Transform)')
    print('\n')

    # Train on log-transformed target
    start_time = time.time()
    model.fit(X_train, y_train_log)
    train_time = time.time() - start_time


    y_train_pred_log = model.predict(X_train)
    y_train_pred = np.expm1(y_train_pred_log)
    y_train_original = np.expm1(y_train_log)
    train_mae = mean_absolute_error(y_train_original, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train_original, y_train_pred))
    train_r2 = r2_score(y_train_original, y_train_pred)
    # Predict in log space
    y_test_pred_log = model.predict(X_test)

    # Transform back to original scale
    y_test_pred = np.expm1(y_test_pred_log)

    # Metrics on original scale
    test_mae = mean_absolute_error(y_test_original, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test_original, y_test_pred))
    test_r2 = r2_score(y_test_original, y_test_pred)

    print(f'Training time: {train_time:.2f}s')
    print(f'Train MAE: {train_mae:.2f}')
    print(f'\nTest metric:')
    print(f'  MAE:  {test_mae:.2f}')
    print(f'Train RMSE: {train_rmse:.2f}')
    print(f'  RMSE: {test_rmse:.2f}')
    print(f'Train RÂ²: {train_r2:.4f}')
    print(f'  R**2:   {test_r2:.4f}')

    return {
        'Model': model_name + ' (Log)',
        'Train_MAE': train_mae,
        'Test_MAE': test_mae,
        'Train_RMSE': train_rmse,
        'Test_RMSE': test_rmse,
        'Train_R2': train_r2,
        'Test_R2': test_r2,
        'Time': train_time,
        'model_object': model
    }

print('Log evaluation function ready')

# Ridge with log transform
ridge_log = Ridge(alpha=1.0, random_state=42)
result_ridge_log = evaluate_model_log(ridge_log, X_train, X_test, y_train_log, y_test_log, y_test, 'Ridge Regression')
all_results.append(result_ridge_log)

# Random Forest with log transform
rf_log = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
result_rf_log = evaluate_model_log(rf_log, X_train, X_test, y_train_log, y_test_log, y_test, 'Random Forest')
all_results.append(result_rf_log)

# XGBoost with log transform
xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=1.0,
    colsample_bytree=1.0,
    colsample_bynode=1.0,
    colsample_bylevel=1.0,
    gamma=0,
    reg_lambda=1,
    reg_alpha=0,
    random_state=42,
    seed=42,
    n_jobs=-1
)

result_xgb = evaluate_model_log(xgb_model, X_train, X_test, y_train_log, y_test_log, y_test, 'XGBoost')
all_results.append(result_xgb)

# Random Forest tuning (Breiman, 2001)


rf_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [4, 8],
    'min_samples_split': [2, 5],
    'max_features': ['auto', 'sqrt']
}

rf_search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    param_distributions=rf_param_grid,
    n_iter=10,
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=1,
    random_state=42
)
rf_search.fit(X_train, y_train_log)
print("Best RF params:", rf_search.best_params_)
print("Best CV MAE:", -rf_search.best_score_)
best_rf_model = rf_search.best_estimator_

# XGBoost tuning (Chen & Guestrin, 2016)


xgb_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1]

}

xgb_search = RandomizedSearchCV(
    XGBRegressor(objective='reg:squarederror', random_state=42),
    param_distributions=xgb_param_grid,
    n_iter=10,
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=2,
    random_state=42
)
xgb_search.fit(X_train, y_train_log)
print("Best XGB params:", xgb_search.best_params_)
print("Best CV MAE:", -xgb_search.best_score_)
best_xgb_model = xgb_search.best_estimator_


cv_scores_rf = cross_val_score(best_rf_model, X_train, y_train_log, cv=5, scoring='neg_mean_absolute_error')
print(f"Random Forest CV MAE: {-cv_scores_rf.mean():.2f} (+/- {cv_scores_rf.std():.2f})")

cv_scores_xgb = cross_val_score(best_xgb_model, X_train, y_train_log, cv=5, scoring='neg_mean_absolute_error')
print(f"XGBoost CV MAE: {-cv_scores_xgb.mean():.2f} (+/- {cv_scores_xgb.std():.2f})")


#  Random Forest tuned evaluation
result_rf_tuned = evaluate_model_log(
    best_rf_model, X_train, X_test, y_train_log, y_test_log, y_test,
    'Random Forest (Tuned)'
)
all_results.append(result_rf_tuned)

# XGBoost tuned evaluation
result_xgb_tuned = evaluate_model_log(
    best_xgb_model, X_train, X_test, y_train_log, y_test_log, y_test,
    'XGBoost (Tuned)'
)
all_results.append(result_xgb_tuned)

print("\n Models tuned on final comparison")
