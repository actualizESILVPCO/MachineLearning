# -*- coding: utf-8 -*-
"""Machine_learning_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fybUcS7DP-OBnCv9pbBbxaER8Q1ZMuKq

# Machine Learning Project: Allstate Claims Severity Prediction

**Team Members:** Pauline Comvopoulos, Léa Dantec, Eva Carrard

**Group:** ACT1

## Table of Contents

1. [Business Case & Context](#business)
2. [Data Loading & Exploration](#data)
3. [Exploratory Data Analysis](#eda)
4. [Data Preprocessing](#preprocessing)
5. [Problem Formalization](#problem)
6. [Baseline Models](#baseline)
7. [Advanced Models](#advanced)
8. [Model Comparison & Results](#results)
9. [Conclusion & Recommendations](#conclusion)
10. [References](#references)

<a id='business'></a>
# 1. BUSINESS CASE & CONTEXT

## 1.1 Problem Statement

Insurance companies face a critical challenge: accurately predicting claim costs before they are finalized. For Allstate, one of the largest insurance providers in the United States, this prediction capability directly impacts:

- **Premium Pricing**: Setting competitive yet profitable insurance rates
- **Reserve Allocation**: Maintaining adequate financial reserves for future claims
- **Risk Management**: Identifying high-cost claims early for proactive management
- **Operational Efficiency**: Optimizing claim handling resources

## 1.2 Business Objective

The primary objective is to develop a machine learning model that predicts the severity (cost) of insurance claims based on claim characteristics. Accurate predictions enable Allstate to:

1. Price policies more accurately, reducing adverse selection
2. Optimize reserve requirements, improving capital efficiency
3. Detect potentially fraudulent or anomalous claims
4. Streamline claim processing workflows

## 1.3 Link to Specialization

This project directly relates to **Data Science and Financial Engineering** through:

- **Predictive Analytics**: Core data science techniques for regression problems
- **Risk Quantification**: Financial modeling of claim distributions
- **Actuarial Science**: Statistical methods used in insurance pricing
- **Machine Learning in Finance**: Application of ML to financial decision-making

## 1.4 Dataset Source

**Source**: Kaggle - Allstate Claims Severity Competition

**URL**: https://www.kaggle.com/c/allstate-claims-severity

**Description**: The dataset contains anonymized features representing claim characteristics and a continuous target variable representing claim cost (loss). Features have been transformed and anonymized for confidentiality.

## 1.5 Success Criteria

The model will be evaluated primarily on:

- **Mean Absolute Error (MAE)**: Primary metric - measures average prediction error
- **Root Mean Squared Error (RMSE)**: Penalizes large errors more heavily
- **R^2 Score**: Measures proportion of variance explained

Target: Achieve MAE significantly better than baseline (mean prediction)

<a id='data'></a>
# 2. DATA LOADING & EXPLORATION
"""

!pip install xgboost --upgrade

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import time
import warnings
import xgboost as xgb
from xgboost import XGBRegressor

# Preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV


# Models


from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor


# Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#We want to fix our values to be in accordance with our report
import random
import os

SEED = 42

# Python
random.seed(SEED)

# Numpy
np.random.seed(SEED)

# OS
os.environ['PYTHONHASHSEED'] = str(SEED)

# XGBoost

xgb.set_config(verbosity=0)

# Scikit-learn
from sklearn import set_config
set_config(print_changed_only=True)

# Load dataset
try:
    df = pd.read_csv('train.csv')
except FileNotFoundError:
    print(" 'train.csv' doesn't exist or not the good name")
    raise

print(f'Dataset shape: {df.shape[0]} rows x {df.shape[1]} columns')
print(f'\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')

df.head()

# Basic information
df.info()

# Remove ID column (not useful for prediction)
if 'id' in df.columns:
    df = df.drop('id', axis=1)
    print('ID column removed')

print(f'Final shape: {df.shape}')

# Identify feature types
categorical_features = df.select_dtypes(include=['object']).columns.tolist()
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Remove target from numerical features
target = 'loss'
if target in numerical_features:
    numerical_features.remove(target)

print(f'Categorical features: {len(categorical_features)}')
print(f'Numerical features: {len(numerical_features)}')
print(f'Target variable: {target}')

# Check for missing values
missing = df.isnull().sum()
if missing.sum() > 0:
    print('Missing values found:')
    print(missing[missing > 0])
else:
    print('No missing values in the dataset ')

"""<a id='eda'></a>
# 3. EXPLORATORY DATA ANALYSIS

## 3.1 Target Variable Analysis
"""

# Target statistics
print('Target Variable Statistics:')
print('\n')
print(df[target].describe())
print(f'\nSkewness: {df[target].skew():.2f}')
print(f'Kurtosis: {df[target].kurtosis():.2f}')

"""# Outliers & Duplicates Detection"""

# Check for duplicates and outliers in the target variable "loss"
print(f'Number of duplicated rows: {df.duplicated().sum()}')
df = df.drop_duplicates()

Q1 = df['loss'].quantile(0.25)
Q3 = df['loss'].quantile(0.75)
IQR = Q3 - Q1
outlier_threshold = Q3 + 3*IQR
print(f'Number of outliers in loss: {(df["loss"] > outlier_threshold).sum()}')
df = df[df['loss'] <= outlier_threshold]

"""### Rationale for Outlier Removal
Outliers in the loss variable can heavily distort model training, especially for linear models and error metrics such as MAE and RMSE.  
Using a 3×IQR rule ensures that we remove only extreme anomalies while preserving the natural variability of insurance losses.  
This step stabilizes training, reduces noise, and improves generalization.

"""

# Visualize target distribution
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Original distribution
axes[0].hist(df[target], bins=50, edgecolor='black', alpha=0.7)
axes[0].axvline(df[target].mean(), color='red', linestyle='--', linewidth=2, label='Mean')
axes[0].axvline(df[target].median(), color='green', linestyle='--', linewidth=2, label='Median')
axes[0].set_xlabel('Loss')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Original Distribution')
axes[0].legend()

# Boxplot
axes[1].boxplot(df[target])
axes[1].set_ylabel('Loss')
axes[1].set_title('Boxplot - Outliers Detection')

# Log transformation
axes[2].hist(np.log1p(df[target]), bins=50, edgecolor='black', alpha=0.7, color='orange')
axes[2].set_xlabel('log(1+Loss)')
axes[2].set_ylabel('Frequency')
axes[2].set_title('Log-Transformed Distribution')

plt.tight_layout()
plt.show()

print('Key Observation: Target is highly right-skewed → Log transformation recommended')

"""### Why Apply a Log Transformation?
The loss distribution is highly right-skewed, which violates core assumptions of many regression algorithms.  
Applying log(1+x) reduces skewness, compresses extreme values, and makes the target more Gaussian-like.  
This typically leads to:
- more stable model training,
- faster convergence,
- improved predictive accuracy,
- reduced sensitivity to large claims.

## 3.2 Numerical Features Analysis
"""

# Summary statistics for numerical features
print('Numerical Features Summary:')
df[numerical_features].describe().T

"""## 3.3 Correlation Analysis"""

# Correlation with target
if len(numerical_features) > 0:
    correlations = df[numerical_features + [target]].corr()[target].sort_values(ascending=False)

    print('Top 10 Features Correlated with Target:')
    print('\n')
    print(correlations.head(11))  # 11 to include target itself

# Correlation heatmap (top features only)
if len(numerical_features) > 0:
    top_features = correlations.abs().sort_values(ascending=False).head(13).index.tolist()

    plt.figure(figsize=(10, 8))
    sns.heatmap(df[top_features].corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0,
                square=True, linewidths=1)
    plt.title('Correlation Matrix - Top Features')
    plt.tight_layout()
    plt.show()

"""<a id='preprocessing'></a>
# 4. DATA PREPROCESSING

---

## 4.1 Handle Missing Values

## 4.2 Encode Categorical Variables
"""

# Create a copy for preprocessing
df_processed = df.copy()

# Handle missing values if any
if df_processed.isnull().sum().sum() > 0:
    print('Handling missing values')

    # fill with median
    for col in numerical_features:
        if df_processed[col].isnull().sum() > 0:
            median_val = df_processed[col].median()
            df_processed[col].fillna(median_val, inplace=True)
            print(f'  {col}: filled with median = {median_val:.2f}')

    #  fill with mode
    for col in categorical_features:
        if df_processed[col].isnull().sum() > 0:
            mode_val = df_processed[col].mode()[0]
            df_processed[col].fillna(mode_val, inplace=True)
            print(f'  {col}: filled with mode = {mode_val}')
else:
    print('No missing values to handle ')

print(f'\nVerification: {df_processed.isnull().sum().sum()} missing values remaining')

print(f' {len(categorical_features)} categorical features')

label_encoders = {}
for feature in categorical_features:
    le = LabelEncoder()
    df_processed[feature] = le.fit_transform(df_processed[feature].astype(str))
    label_encoders[feature] = le
    print(f'  {feature}: {len(le.classes_)} categories encoded')

"""# Dimensionality Reduction - PCA & SelectKBest

We considered techniques like PCA and SelectKBest, but chose not to apply them. Our models (RF & XGBoost) handle feature selection natively, and our data size made overfitting unlikely. We prioritized tuning and optimization for better results.

## 4.3 Split Features and Target

## 4.4 Feature Scaling

### Why Feature Scaling is Necessary
Although tree-based models are scale-invariant, linear models (Linear Regression, Ridge) and gradient boosting methods benefit from standardized inputs.  
Scaling ensures:
- comparable feature magnitudes,
- improved numerical stability,
- more reliable regularization behavior,
- better convergence for gradient-based solvers.

## 4.5 Train/Test Split
"""

# 1. Split the full dataset before separation or transformation , 80% train, 20% test
train_df, test_df = train_test_split(
    df_processed, test_size=0.2, random_state=42
)

print("Data Split:")
print(f"Training set: {train_df.shape[0]} samples ({train_df.shape[0]/len(df_processed)*100:.1f}%)")
print(f"Test set: {test_df.shape[0]} samples ({test_df.shape[0]/len(df_processed)*100:.1f}%)")
print(f"Nombre de colonnes: {train_df.shape[1]}")

# 2. Separate X / y after the split
X_train = train_df.drop(columns=[target])
y_train = train_df[target]

X_test = test_df.drop(columns=[target])
y_test = test_df[target]

print("\nSéparation après split :")
print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

# 3. Standardization (fit on train only, transform on test)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)

print("\nStandardisation réalisée (fit sur train, transform sur test).")

# 4. Log transformation of the target variable
print("\nLog transformation du target :")
print(f"Before transformation - y_train range: [{y_train.min():.2f}, {y_train.max():.2f}]")

y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)

print(f"After transformation - y_train_log range: [{y_train_log.min():.2f}, {y_train_log.max():.2f}]")
print(f"Skewness reduce: {y_train.skew():.2f} -> {y_train_log.skew():.2f}")

"""<a id='problem'></a>
# 5. PROBLEM FORMALIZATION

"""

# Separate X and y
X = df_processed.drop(target, axis=1)
y = df_processed[target]

print(f'Features (X): {X.shape}')
print(f'Target (y): {y.shape}')
print(f'\nFeature names: {X.columns.tolist()[:10]}...')

"""## Problem Type

**Supervised Regression**

## Objective

Predict the severity (cost) of insurance claims based on anonymized claim characteristics.

## Business Context

Allstate needs accurate claim cost predictions to:
- Set appropriate insurance premiums
- Allocate financial reserves efficiently
- Identify high-risk claims early
- Optimize claim handling resources

## Variables

- **Target (y):** `loss` - claim cost (continuous variable, highly skewed)
- **Features (X):** 130 anonymized variables (mix of categorical and numerical)

## Main Challenges

1. **Highly skewed target distribution** → Log transformation may help
2. **Many categorical features** → Proper encoding required
3. **Anonymized features** → Difficult to create domain-specific features
4. **Potential outliers** → Robust models preferred
5. **High dimensionality** → Risk of overfitting

## Evaluation Metrics

- **MAE (Mean Absolute Error)**: Primary metric for this competition
- **RMSE (Root Mean Squared Error)**: Penalizes large errors more
- **R² (R-squared)**: Measures proportion of variance explained

## Modeling Strategy

1. Start with simple baseline models (Linear, Ridge)
2. Try tree-based models (Random Forest)
3. Consider log transformation of target
4. Implement ensemble methods for improvement
5. Tune hyperparameters for best performance

<a id='baseline'></a>
# 6. BASELINE MODELS

## 6.1 Model Selection

We will test three baseline models:

1. **Linear Regression**: Simplest baseline with no regularization
2. **Ridge Regression**: Linear model with L2 regularization to prevent overfitting
3. **Random Forest**: Tree-based ensemble that can capture non-linear relationships

These models provide a good range from simple to moderately complex, allowing us to assess whether linear or non-linear approaches work better for this problem.

### Evaluation Strategy
We evaluate all models using MAE, RMSE, and R**2 to capture complementary aspects of performance.  
MAE reflects average prediction error, RMSE penalizes large mistakes, and R**2 measures explained variance.  
Using the same evaluation protocol for all models ensures a fair and rigorous comparison.
"""

# Function to evaluate models
def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    print('\n')
    print(f'{model_name}')
    print('\n')

    # Train
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # Predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Metrics
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_r2 = r2_score(y_train, y_train_pred)

    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_r2 = r2_score(y_test, y_test_pred)

    print(f'Training time: {train_time:.2f}s')
    print(f'\nTRAIN METRICS:')
    print(f'  MAE:  {train_mae:.2f}')
    print(f'  RMSE: {train_rmse:.2f}')
    print(f'  R**2:   {train_r2:.4f}')

    print(f'\nTEST METRICS:')
    print(f'  MAE:  {test_mae:.2f}')
    print(f'  RMSE: {test_rmse:.2f}')
    print(f'  R**2:   {test_r2:.4f}')

    return {
        'Model': model_name,
        'Train_MAE': train_mae,
        'Test_MAE': test_mae,
        'Train_RMSE': train_rmse,
        'Test_RMSE': test_rmse,
        'Train_R2': train_r2,
        'Test_R2': test_r2,
        'Time': train_time,
        'model_object': model
    }

"""## 6.2 Training Baseline Models"""

# Store results
all_results = []

# Model 1: Linear Regression
lr_model = LinearRegression()
result_lr_model = evaluate_model(lr_model, X_train, X_test, y_train, y_test, 'Linear Regression')
all_results.append(result_lr_model)

# Model 2: Ridge Regression
ridge = Ridge(alpha=1.0, random_state=42)
result_ridge = evaluate_model(ridge, X_train, X_test, y_train, y_test, 'Ridge Regression')
all_results.append(result_ridge)

# Model 3: Random Forest
rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
result_rf = evaluate_model(rf, X_train, X_test, y_train, y_test, 'Random Forest')
all_results.append(result_rf)

"""## 6.3 Baseline Results Comparison"""

# Create comparison dataframe
results_df = pd.DataFrame([{k: v for k, v in r.items() if k != 'model_object'} for r in all_results])


print('BASELINE MODELS COMPARISON')
print('\n')
print(results_df.to_string(index=False))

# Identify best model
best_idx = results_df['Test_MAE'].idxmin()
best_model_name = results_df.loc[best_idx, 'Model']
print(f'\nBest baseline model (by MAE): {best_model_name}')

"""## 6.4 Visualizations"""

# Get predictions from best model
best_model = all_results[best_idx]['model_object']
y_pred = best_model.predict(X_test)

# Create visualizations
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Actual vs Predicted
axes[0].scatter(y_test, y_pred, alpha=0.5, s=10)
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual Loss')
axes[0].set_ylabel('Predicted Loss')
axes[0].set_title(f'{best_model_name}: Predictions vs Actual')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Residuals
residuals = y_test - y_pred
axes[1].scatter(y_pred, residuals, alpha=0.5, s=10)
axes[1].axhline(0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('Predicted Loss')
axes[1].set_ylabel('Residuals')
axes[1].set_title(f'{best_model_name}: Residual Plot')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Error distribution
errors = np.abs(y_test - y_pred)

plt.figure(figsize=(10, 5))
plt.hist(errors, bins=50, edgecolor='black', alpha=0.7)
plt.axvline(errors.mean(), color='red', linestyle='--', linewidth=2,
            label=f'Mean Error: {errors.mean():.2f}')
plt.axvline(errors.median(), color='green', linestyle='--', linewidth=2,
            label=f'Median Error: {errors.median():.2f}')
plt.xlabel('Absolute Error')
plt.ylabel('Frequency')
plt.title(f'{best_model_name}: Error Distribution')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f'Error Statistics:')
print(f'  Mean: {errors.mean():.2f}')
print(f'  Median: {errors.median():.2f}')
print(f'  Std: {errors.std():.2f}')
print(f'  Max: {errors.max():.2f}')

"""<a id='advanced'></a>
# 7. ADVANCED MODELS

## 7.1 Log Transformation of Target

Since the target variable is highly skewed, we will try log transformation which often improves model performance for skewed distributions.
"""

# Apply log transformation
y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)

print('Log transformation applied to target variable')

print(f'Log-transformed skewness: {y_train_log.skew():.2f}')

"""## 7.2 Models with Log-Transformed Target"""

# Function to evaluate with log transformation
def evaluate_model_log(model, X_train, X_test, y_train_log, y_test_log, y_test_original, model_name):
    print('\n')
    print(f'{model_name} (with Log Transform)')
    print('\n')

    # Train on log-transformed target
    start_time = time.time()
    model.fit(X_train, y_train_log)
    train_time = time.time() - start_time


    y_train_pred_log = model.predict(X_train)
    y_train_pred = np.expm1(y_train_pred_log)
    y_train_original = np.expm1(y_train_log)
    train_mae = mean_absolute_error(y_train_original, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train_original, y_train_pred))
    train_r2 = r2_score(y_train_original, y_train_pred)
    # Predict in log space
    y_test_pred_log = model.predict(X_test)

    # Transform back to original scale
    y_test_pred = np.expm1(y_test_pred_log)

    # Metrics on original scale
    test_mae = mean_absolute_error(y_test_original, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test_original, y_test_pred))
    test_r2 = r2_score(y_test_original, y_test_pred)

    print(f'Training time: {train_time:.2f}s')
    print(f'Train MAE: {train_mae:.2f}')
    print(f'\nTest metric:')
    print(f'  MAE:  {test_mae:.2f}')
    print(f'Train RMSE: {train_rmse:.2f}')
    print(f'  RMSE: {test_rmse:.2f}')
    print(f'Train R²: {train_r2:.4f}')
    print(f'  R**2:   {test_r2:.4f}')

    return {
        'Model': model_name + ' (Log)',
        'Train_MAE': train_mae,
        'Test_MAE': test_mae,
        'Train_RMSE': train_rmse,
        'Test_RMSE': test_rmse,
        'Train_R2': train_r2,
        'Test_R2': test_r2,
        'Time': train_time,
        'model_object': model
    }

print('Log evaluation function ready')

# Ridge with log transform
ridge_log = Ridge(alpha=1.0, random_state=42)
result_ridge_log = evaluate_model_log(ridge_log, X_train, X_test, y_train_log, y_test_log, y_test, 'Ridge Regression')
all_results.append(result_ridge_log)

# Random Forest with log transform
rf_log = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
result_rf_log = evaluate_model_log(rf_log, X_train, X_test, y_train_log, y_test_log, y_test, 'Random Forest')
all_results.append(result_rf_log)

"""## 7.3 XGBoost Model

XGBoost is a gradient boosting algorithm that often performs well on structured data. It has been referenced in numerous machine learning competitions and academic papers for its performance.

**Reference**: Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
"""

# XGBoost with log transform
xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=1.0,
    colsample_bytree=1.0,
    colsample_bynode=1.0,
    colsample_bylevel=1.0,
    gamma=0,
    reg_lambda=1,
    reg_alpha=0,
    random_state=42,
    seed=42,
    n_jobs=-1
)

result_xgb = evaluate_model_log(xgb_model, X_train, X_test, y_train_log, y_test_log, y_test, 'XGBoost')
all_results.append(result_xgb)

"""# Hyperparameter Tuning - Random Forest & XGBoost"""

# Random Forest tuning (Breiman, 2001)


rf_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [4, 8],
    'min_samples_split': [2, 5],
    'max_features': ['auto', 'sqrt']
}

rf_search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    param_distributions=rf_param_grid,
    n_iter=10,
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=1,
    random_state=42
)
rf_search.fit(X_train, y_train_log)
print("Best RF params:", rf_search.best_params_)
print("Best CV MAE:", -rf_search.best_score_)
best_rf_model = rf_search.best_estimator_

# XGBoost tuning (Chen & Guestrin, 2016)


xgb_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1]

}

xgb_search = RandomizedSearchCV(
    XGBRegressor(objective='reg:squarederror', random_state=42),
    param_distributions=xgb_param_grid,
    n_iter=10,
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=2,
    random_state=42
)
xgb_search.fit(X_train, y_train_log)
print("Best XGB params:", xgb_search.best_params_)
print("Best CV MAE:", -xgb_search.best_score_)
best_xgb_model = xgb_search.best_estimator_

"""# Cross-validation for Tuned Models"""

cv_scores_rf = cross_val_score(best_rf_model, X_train, y_train_log, cv=5, scoring='neg_mean_absolute_error')
print(f"Random Forest CV MAE: {-cv_scores_rf.mean():.2f} (+/- {cv_scores_rf.std():.2f})")

cv_scores_xgb = cross_val_score(best_xgb_model, X_train, y_train_log, cv=5, scoring='neg_mean_absolute_error')
print(f"XGBoost CV MAE: {-cv_scores_xgb.mean():.2f} (+/- {cv_scores_xgb.std():.2f})")

#  Random Forest tuned evaluation
result_rf_tuned = evaluate_model_log(
    best_rf_model, X_train, X_test, y_train_log, y_test_log, y_test,
    'Random Forest (Tuned)'
)
all_results.append(result_rf_tuned)

# XGBoost tuned evaluation
result_xgb_tuned = evaluate_model_log(
    best_xgb_model, X_train, X_test, y_train_log, y_test_log, y_test,
    'XGBoost (Tuned)'
)
all_results.append(result_xgb_tuned)

print("\n Models tuned on final comparison")

"""<a id='results'></a>
# 8. MODEL COMPARISON & RESULTS


"""

# Final comparison
final_results = pd.DataFrame([{k: v for k, v in r.items() if k != 'model_object'} for r in all_results])

print('\n')
print('Final Model Comparison')
print('\n')
print(final_results.to_string(index=False))

# Best model
best_final_idx = final_results['Test_MAE'].idxmin()
best_final_model = final_results.loc[best_final_idx, 'Model']
best_final_mae = final_results.loc[best_final_idx, 'Test_MAE']

print('\n')
print(f'BEST MODEL: {best_final_model}')
print(f'Test MAE: {best_final_mae:.2f}')
print(f'Test RMSE: {final_results.loc[best_final_idx, "Test_RMSE"]:.2f}')
print(f'Test R**2: {final_results.loc[best_final_idx, "Test_R2"]:.4f}')
print('\n')

# Visualize comparison
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# MAE comparison
axes[0].barh(final_results['Model'], final_results['Test_MAE'])
axes[0].set_xlabel('MAE')
axes[0].set_title('Model Comparison - MAE (lower is better)')
axes[0].invert_yaxis()

# RMSE comparison
axes[1].barh(final_results['Model'], final_results['Test_RMSE'])
axes[1].set_xlabel('RMSE')
axes[1].set_title('Model Comparison - RMSE (lower is better)')
axes[1].invert_yaxis()

# R² comparison
axes[2].barh(final_results['Model'], final_results['Test_R2'])
axes[2].set_xlabel('R**2')
axes[2].set_title('Model Comparison - R² (higher is better)')
axes[2].invert_yaxis()

plt.tight_layout()
plt.show()

"""# Feature Importance (XGBoost)"""

importances = best_xgb_model.feature_importances_
indices = np.argsort(importances)[::-1]
feature_names = X_train.columns

plt.figure(figsize=(10, 6))
plt.title("Feature Importances (XGBoost)")
plt.bar(range(20), importances[indices][:20], align='center')
plt.xticks(range(20), feature_names[indices][:20], rotation=45)
plt.show()

print('Top 10 important features:', feature_names[indices][:10].tolist())

"""<a id='conclusion'></a>
# 9. CONCLUSION & RECOMMENDATIONS

## 9.1 Summary of Findings

### Data Analysis
- Dataset contains 188,000+ insurance claims with 130 anonymized features
- Target variable (loss) is highly right-skewed (skewness > 1)
- No missing values in the dataset
- Mix of categorical (116) and numerical (14) features
- Weak correlations between individual features and target

### Preprocessing
- Label encoding applied to all categorical variables
- StandardScaler used for feature normalization
- 80/20 train/test split maintained
- Log transformation of target significantly improved model performance

### Model Performance
- **Baseline models** (without log transform) achieved MAE approximativly arround 1200-1300
- **Log transformation** improved performance across all models
- **Tree-based models** (Random Forest, XGBoost) outperformed linear models
- **Best model** achieved test MAE around 1040



### Key Insights
1. **Log transformation is critical** for this skewed target distribution
2. **Non-linear models perform better** than linear regression
3. **Ensemble methods** (XGBoost) provide best results
4. The anonymized nature of features limits domain-specific feature engineering

## 9.2 Business Recommendations

### For Allstate
1. **Deploy the best model** (XGBoost with log transformation) for production use
2. **Implement continuous monitoring** of model performance on new claims
3. **Set prediction confidence thresholds** for automated vs manual review
4. **Use predictions for**:
   - Premium pricing adjustments
   - Reserve allocation optimization
   - Early identification of high-cost claims

### Risk Management
- Focus manual review on claims where predicted cost exceeds certain thresholds
- Use prediction uncertainty to flag potentially fraudulent claims
- Regularly retrain model with new data to maintain accuracy

## 9.3 Limitations

1. **Anonymized features** prevent business interpretation and domain-driven improvements
2. **Single dataset** - no external validation on different time periods
3. **Model interpretability** - tree-based models are less interpretable than linear models
4. **Computational cost** - XGBoost requires more resources than simpler models

## 9.4 Future Improvements

1. **Hyperparameter tuning**: Use GridSearchCV or Bayesian optimization for XGBoost
2. **Feature engineering**: Create interaction features, polynomial features
3. **Ensemble stacking**: Combine multiple models for improved predictions
4. **Deep learning**: Try neural networks if computational resources allow
5. **Outlier handling**: Investigate and potentially remove extreme outliers
6. **Cross-validation**: Implement k-fold CV for more robust evaluation

## 9.5 Project Conclusion

This project successfully developed a machine learning model to predict insurance claim costs for Allstate. The model achieved significant improvement over baseline predictions and provides actionable insights for business operations. The log transformation of the target variable proved critical for handling the skewed distribution, and gradient boosting methods (XGBoost) delivered the best performance.

The project demonstrates the value of machine learning in financial services, particularly for risk assessment and pricing optimization. With further refinement and deployment, this model can directly contribute to Allstate's operational efficiency and financial performance.
"""