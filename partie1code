
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import time
import warnings
import xgboost as xgb
from xgboost import XGBRegressor

# Preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV


# Models


from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor


# Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#We want to fix our values to be in accordance with our report
import random
import os

SEED = 42

# Python
random.seed(SEED)

# Numpy
np.random.seed(SEED)

# OS
os.environ['PYTHONHASHSEED'] = str(SEED)

# XGBoost

xgb.set_config(verbosity=0)

# Scikit-learn
from sklearn import set_config
set_config(print_changed_only=True)

# Load dataset
try:
    df = pd.read_csv('train.csv')
except FileNotFoundError:
    print(" 'train.csv' doesn't exist or not the good name")
    raise

print(f'Dataset shape: {df.shape[0]} rows x {df.shape[1]} columns')
print(f'\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')

df.head()

# Basic information
df.info()
# Remove ID column (not useful for prediction)
if 'id' in df.columns:
    df = df.drop('id', axis=1)
    print('ID column removed')

print(f'Final shape: {df.shape}')

# Identify feature types
categorical_features = df.select_dtypes(include=['object']).columns.tolist()
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Remove target from numerical features
target = 'loss'
if target in numerical_features:
    numerical_features.remove(target)

print(f'Categorical features: {len(categorical_features)}')
print(f'Numerical features: {len(numerical_features)}')
print(f'Target variable: {target}')

# Check for missing values
missing = df.isnull().sum()
if missing.sum() > 0:
    print('Missing values found:')
    print(missing[missing > 0])
else:
    print('No missing values in the dataset ')
# Target statistics
print('Target Variable Statistics:')
print('\n')
print(df[target].describe())
print(f'\nSkewness: {df[target].skew():.2f}')
print(f'Kurtosis: {df[target].kurtosis():.2f}')

# Check for duplicates and outliers in the target variable "loss"
print(f'Number of duplicated rows: {df.duplicated().sum()}')
df = df.drop_duplicates()

Q1 = df['loss'].quantile(0.25)
Q3 = df['loss'].quantile(0.75)
IQR = Q3 - Q1
outlier_threshold = Q3 + 3*IQR
print(f'Number of outliers in loss: {(df["loss"] > outlier_threshold).sum()}')
df = df[df['loss'] <= outlier_threshold]
# Visualize target distribution
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Original distribution
axes[0].hist(df[target], bins=50, edgecolor='black', alpha=0.7)
axes[0].axvline(df[target].mean(), color='red', linestyle='--', linewidth=2, label='Mean')
axes[0].axvline(df[target].median(), color='green', linestyle='--', linewidth=2, label='Median')
axes[0].set_xlabel('Loss')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Original Distribution')
axes[0].legend()

# Boxplot
axes[1].boxplot(df[target])
axes[1].set_ylabel('Loss')
axes[1].set_title('Boxplot - Outliers Detection')

# Log transformation
axes[2].hist(np.log1p(df[target]), bins=50, edgecolor='black', alpha=0.7, color='orange')
axes[2].set_xlabel('log(1+Loss)')
axes[2].set_ylabel('Frequency')
axes[2].set_title('Log-Transformed Distribution')

plt.tight_layout()
plt.show()

print('Key Observation: Target is highly right-skewed â†’ Log transformation recommended')

# Summary statistics for numerical features
print('Numerical Features Summary:')
df[numerical_features].describe().T

# Correlation with target
if len(numerical_features) > 0:
    correlations = df[numerical_features + [target]].corr()[target].sort_values(ascending=False)

    print('Top 10 Features Correlated with Target:')
    print('\n')
    print(correlations.head(11))  # 11 to include target itself
# Correlation heatmap (top features only)
if len(numerical_features) > 0:
    top_features = correlations.abs().sort_values(ascending=False).head(13).index.tolist()

    plt.figure(figsize=(10, 8))
    sns.heatmap(df[top_features].corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0,
                square=True, linewidths=1)
    plt.title('Correlation Matrix - Top Features')
    plt.tight_layout()
    plt.show()
    
# Create a copy for preprocessing
df_processed = df.copy()

# Handle missing values if any
if df_processed.isnull().sum().sum() > 0:
    print('Handling missing values')

    # fill with median
    for col in numerical_features:
        if df_processed[col].isnull().sum() > 0:
            median_val = df_processed[col].median()
            df_processed[col].fillna(median_val, inplace=True)
            print(f'  {col}: filled with median = {median_val:.2f}')

    #  fill with mode
    for col in categorical_features:
        if df_processed[col].isnull().sum() > 0:
            mode_val = df_processed[col].mode()[0]
            df_processed[col].fillna(mode_val, inplace=True)
            print(f'  {col}: filled with mode = {mode_val}')
else:
    print('No missing values to handle ')

print(f'\nVerification: {df_processed.isnull().sum().sum()} missing values remaining')



print(f' {len(categorical_features)} categorical features')

label_encoders = {}
for feature in categorical_features:
    le = LabelEncoder()
    df_processed[feature] = le.fit_transform(df_processed[feature].astype(str))
    label_encoders[feature] = le
    print(f'  {feature}: {len(le.classes_)} categories encoded')

# Separate X and y
X = df_processed.drop(target, axis=1)
y = df_processed[target]

print(f'Features (X): {X.shape}')
print(f'Target (y): {y.shape}')
print(f'\nFeature names: {X.columns.tolist()[:10]}...')



# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)



# Split data: 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print('Data Split:')
print('\n')
print(f'Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)')
print(f'Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)')
print(f'Number of features: {X_train.shape[1]}')



print("log transformation")
print(f"Before transformation - y_train range: [{y_train.min():.2f}, {y_train.max():.2f}]")

# Transformation log
y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)

print(f"After transformation - y_train_log range: [{y_train_log.min():.2f}, {y_train_log.max():.2f}]")
print(f"Skewness reduce: {y_train.skew():.2f} -> {y_train_log.skew():.2f}")

